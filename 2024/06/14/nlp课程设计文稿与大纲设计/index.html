<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="https://boots-coder.github.io">
  <title>nlp课程设计 文稿与大纲设计 | coderboots</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="nlp 课程设计GloVE概述与引言：Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origi">
<meta property="og:type" content="article">
<meta property="og:title" content="nlp课程设计 文稿与大纲设计">
<meta property="og:url" content="https://boots-coder.github.io/2024/06/14/nlp%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%96%87%E7%A8%BF%E4%B8%8E%E5%A4%A7%E7%BA%B2%E8%AE%BE%E8%AE%A1/index.html">
<meta property="og:site_name" content="coderboots">
<meta property="og:description" content="nlp 课程设计GloVE概述与引言：Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613202142315.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613192113460.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613192639091.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613192939546.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613211037513.png">
<meta property="og:image" content="https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-379633f727a3768188076980b4e4a3a2_l3.svg">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613210222568.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613210204537.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614204612026.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614202023757.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614194336699.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614195527537.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614200620628.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615191139130.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615183425785.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615193424977.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615193404410.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615202736646.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616010054527.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616012907074.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616013045329.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615180535827.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616174343675.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616175243506.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616180109384.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616201225380.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616201132315.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617041934398.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617043134155.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617043210161.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617044017009.png">
<meta property="og:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617043935298.png">
<meta property="article:published_time" content="2024-06-14T13:35:07.000Z">
<meta property="article:modified_time" content="2024-06-18T08:01:40.320Z">
<meta property="article:tag" content="nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613202142315.png">
  
    <link rel="alternative" href="/atom.xml" title="coderboots" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: #4d4d4d;
    }
  </style>
  

  

<meta name="generator" content="Hexo 7.2.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/img/horse-fly.png" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/tags/%E6%97%A5%E8%AE%B0/">日记</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/boots-coder" title="github"><i class="icon-github"></i></a>
		        
					<a class="mail" target="_blank" href="/jiahaoxuan000@gmail.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/img/horse-fly.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/boots-coder" title="github"><i class="icon-github"></i></a>
			        
						<a class="mail" target="_blank" href="/jiahaoxuan000@gmail.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/">主页</a></li>
		        
					<li style="width: 50%"><a href="/tags/%E6%97%A5%E8%AE%B0/">日记</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-nlp课程设计文稿与大纲设计" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      nlp课程设计 文稿与大纲设计
    </h1>
  

        
        <a href="/2024/06/14/nlp%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%96%87%E7%A8%BF%E4%B8%8E%E5%A4%A7%E7%BA%B2%E8%AE%BE%E8%AE%A1/" class="archive-article-date">
  	<time datetime="2024-06-14T13:35:07.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2024-06-14</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="nlp-课程设计"><a href="#nlp-课程设计" class="headerlink" title="nlp 课程设计"></a>nlp 课程设计</h1><h2 id="GloVE"><a href="#GloVE" class="headerlink" title="GloVE"></a>GloVE</h2><h3 id="概述与引言："><a href="#概述与引言：" class="headerlink" title="概述与引言："></a>概述与引言：</h3><p>Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new &#x3D;&#x3D;global log-bilinear regression model&#x3D;&#x3D; that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by &#x3D;&#x3D;training only on the nonzero elements in a word-word co-occurrence matrix&#x3D;&#x3D;, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.</p>
<span id="more"></span>

<h3 id="基本思想与优势："><a href="#基本思想与优势：" class="headerlink" title="基本思想与优势："></a>基本思想与优势：</h3><p>The two main model families for learning word vectors are: </p>
<ol>
<li>global matrix factorization methods, such as latent semantic analysis (LSA) (Deerwester et al., 1990) and</li>
<li>local context window methods, such as the skip-gram model of Mikolov et al. (2013c).</li>
</ol>
<p>Currently, both families suffer significant drawbacks.</p>
<p> While methods like LSA efficiently leverage statistical information, they do relatively &#x3D;&#x3D;poorly on the word analogy task&#x3D;&#x3D;, indicating a sub-optimal vector space structure. </p>
<p>Methods like skip-gram may do better on the analogy task, but they&#x3D;&#x3D;poorly utilize the statistics of the corpus&#x3D;&#x3D; since they train on separate local context windows instead of on global co-occurrence counts.</p>
<p>In this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression models are appropriate for doing so. We propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model produces a word vector space with meaningful substructure, as evidenced by its state-of-the-art performance of 75% accuracy on the word analogy dataset. We also demonstrate that our methods outperform other current methods on several word similarity tasks, and also on a common named entity recognition (NER) benchmark.</p>
<p>引用： <a target="_blank" rel="noopener" href="https://nlp.stanford.edu/pubs/glove.pdf">https://nlp.stanford.edu/pubs/glove.pdf</a></p>
<h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p>Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as LSA. These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the matrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus.</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613202142315.png" alt="image-20240613202142315"></p>
<p>A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with “the” or “and”, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness.</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613192113460.png" alt="image-20240613192113460"></p>
<p>A number of techniques exist that address this shortcoming of HAL, such as the COALS method (Rohde et al., 2006), in which the co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations.</p>
<p>假设一个合理规模的语料库中，某些词对的共现次数范围在1到1亿之间（即 10010^0100 到 10810^8108），跨越了8个数量级。</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613192639091.png" alt="image-20240613192639091"></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613192939546.png" alt="image-20240613192939546"></p>
<p>Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models.</p>
<p>Recently, the importance of the full neural network structure for learning useful word representations has been called into question. </p>
<p>The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. </p>
<p>Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors.</p>
<p> Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613211037513.png" alt="image-20240613211037513"></p>
<h3 id="核心思想与数学推导："><a href="#核心思想与数学推导：" class="headerlink" title="核心思想与数学推导："></a>核心思想与数学推导：</h3><p>GloVe（Global Vectors for Word Representation）的数学推理部分主要包括以下几个步骤：</p>
<ol>
<li><p><strong>共现矩阵的构建</strong>：根据语料库构建一个共现矩阵 (X)，其中元素 (X_{ij}) 表示单词 (i) 和上下文单词 (j) 在特定上下文窗口内共同出现的次数。</p>
</li>
<li><p><strong>近似关系的公式</strong>：提出词向量与共现矩阵之间的近似关系：<br>$$<br>w_i^T \tilde{w_j} + b_i + \tilde{b_j} &#x3D; \log(X_{ij})<br>$$<br>其中， (w_i) 和 (\tilde{w_j}) 是词向量， (b_i) 和 (\tilde{b_j}) 是偏置项。</p>
</li>
<li><p><strong>损失函数的构建</strong>：<br>$$<br>J &#x3D; \sum_{i,j&#x3D;1}^{V} f(X_{ij}) (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - \log(X_{ij}))^2<br>$$<br>损失函数使用了一个加权函数 (f(X_{ij}))，其目的是对频繁共现的单词赋予更大的权重，同时避免权重过大。</p>
</li>
<li><p><strong>权重函数的形式</strong>：选择分段函数形式的权重函数：<br>$$<br>f(x) &#x3D; \begin{cases}<br>(x&#x2F;x_{max})^\alpha &amp; \text{if } x &lt; x_{max} \<br>1 &amp; \text{otherwise}<br>\end{cases}<br>$$</p>
</li>
</ol>
<p>引用 ：<a target="_blank" rel="noopener" href="https://www.fanyeong.com/2018/02/19/glove-in-detail/">https://www.fanyeong.com/2018/02/19/glove-in-detail/</a></p>
<h3 id="扩展–-Example-of-Co-occurrence-Matrices"><a href="#扩展–-Example-of-Co-occurrence-Matrices" class="headerlink" title="扩展– Example of Co-occurrence Matrices"></a>扩展– Example of Co-occurrence Matrices</h3><p>As an example, let’s use three sentences below:</p>
<blockquote>
<p>Apples are green and red.</p>
<p>Red apples are sweet.</p>
<p>Green oranges are sour.</p>
</blockquote>
<p>Let’s assume that these three sentences are our text corpus, elements are words and their context is one sentence. It means that for each pair of two words from the sentences above, we need to count how many times they appear together in one sentence. For example, the words “apples” and “red” appear two times, in the first and second sentences, while the words “red” and “sour” don’t appear in the same sentence.</p>
<p>Following that logic, our co-occurrence matrix will look like this:</p>
<p><img src="https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-379633f727a3768188076980b4e4a3a2_l3.svg" alt="Rendered by QuickLaTeX.com"></p>
<p>From the table above, we can notice that the co-occurrence matrix is symmetric. It means that the value with row X and column Y will be the same as the value with row Y and column X. <strong>In general, we don’t need to keep all elements from the text corpus in the co-occurrence matrix but only those of interest.</strong></p>
<p><strong>For instance, before creating a matrix, it would be useful to clean text, remove stop words, implement <a target="_blank" rel="noopener" href="https://www.baeldung.com/cs/stemming-vs-lemmatization">stemming and lemmatization</a>, and similar.</strong></p>
<h3 id="扩展–熵归一化的具体操作"><a href="#扩展–熵归一化的具体操作" class="headerlink" title="扩展–熵归一化的具体操作"></a>扩展–熵归一化的具体操作</h3><p>熵归一化是一种通过计算共现矩阵中每个元素的信息熵，对原始计数进行归一化处理的方法。它能够有效地减少高频元素对整体计算的影响。下面通过具体的步骤和例子来说明熵归一化的操作过程。</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li><p><strong>计算共现矩阵</strong>：</p>
<ul>
<li>首先，构建一个词-词共现矩阵 (M)，其中 (M_{ij}) 表示词 (i) 和词 (j) 的共现次数。</li>
</ul>
</li>
<li><p><strong>计算每个词的边际概率</strong>：</p>
<ul>
<li>计算词 (i) 出现的概率 (P(w_i))，即词 (i) 在所有共现中的总出现次数除以所有共现的总次数：<br>$$<br>P(w_i) &#x3D; \frac{\sum_{j} M_{ij}}{\sum_{i} \sum_{j} M_{ij}}<br>$$</li>
</ul>
</li>
<li><p><strong>计算每对词的联合概率</strong>：</p>
<ul>
<li>计算词 (i) 和词 (j) 同时出现的概率 (P(w_i, w_j))，即 (M_{ij}) 除以所有共现的总次数：<br>$$<br>P(w_i, w_j) &#x3D; \frac{M_{ij}}{\sum_{i} \sum_{j} M_{ij}}<br>$$</li>
</ul>
</li>
<li><p><strong>计算信息熵</strong>：</p>
<ul>
<li><p>计算每对词的点互信息（PMI），并应用熵归一化公式：<br>$$<br>PMI(w_i, w_j) &#x3D; \log \frac{P(w_i, w_j)}{P(w_i) P(w_j)}<br>$$</p>
</li>
<li><p>如果PMI值为负，则取0，否则保留正值，即正点互信息（PPMI）：<br>$$<br>PPMI(w_i, w_j) &#x3D; \max(PMI(w_i, w_j), 0)<br>$$</p>
</li>
</ul>
</li>
</ol>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>假设我们有一个简单的语料库，包含以下单词共现信息：</p>
<table>
<thead>
<tr>
<th>词对</th>
<th>共现次数</th>
</tr>
</thead>
<tbody><tr>
<td>(dog, bark)</td>
<td>100</td>
</tr>
<tr>
<td>(dog, run)</td>
<td>50</td>
</tr>
<tr>
<td>(cat, meow)</td>
<td>80</td>
</tr>
<tr>
<td>(cat, run)</td>
<td>40</td>
</tr>
<tr>
<td>(bird, fly)</td>
<td>90</td>
</tr>
<tr>
<td>(bird, chirp)</td>
<td>70</td>
</tr>
</tbody></table>
<p>我们将构建一个3×3的共现矩阵 (M)：</p>
<table>
<thead>
<tr>
<th></th>
<th>dog</th>
<th>cat</th>
<th>bird</th>
</tr>
</thead>
<tbody><tr>
<td>bark</td>
<td>100</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>run</td>
<td>50</td>
<td>40</td>
<td>0</td>
</tr>
<tr>
<td>fly</td>
<td>0</td>
<td>0</td>
<td>90</td>
</tr>
<tr>
<td>meow</td>
<td>0</td>
<td>80</td>
<td>0</td>
</tr>
<tr>
<td>chirp</td>
<td>0</td>
<td>0</td>
<td>70</td>
</tr>
</tbody></table>
<ol>
<li><p><strong>计算每个词的边际概率</strong>：</p>
<ul>
<li>(P(bark) &#x3D; \frac{100}{430} \approx 0.2326)</li>
<li>(P(run) &#x3D; \frac{90}{430} \approx 0.2093)</li>
<li>(P(fly) &#x3D; \frac{90}{430} \approx 0.2093)</li>
<li>(P(meow) &#x3D; \frac{80}{430} \approx 0.1860)</li>
<li>(P(chirp) &#x3D; \frac{70}{430} \approx 0.1628)</li>
</ul>
</li>
<li><p><strong>计算每对词的联合概率</strong>：</p>
<ul>
<li>(P(dog, bark) &#x3D; \frac{100}{430} \approx 0.2326)</li>
<li>(P(dog, run) &#x3D; \frac{50}{430} \approx 0.1163)</li>
<li>(P(cat, meow) &#x3D; \frac{80}{430} \approx 0.1860)</li>
<li>(P(cat, run) &#x3D; \frac{40}{430} \approx 0.0930)</li>
<li>(P(bird, fly) &#x3D; \frac{90}{430} \approx 0.2093)</li>
<li>(P(bird, chirp) &#x3D; \frac{70}{430} \approx 0.1628)</li>
</ul>
</li>
<li><p><strong>计算PMI并应用熵归一化</strong>：</p>
<ul>
<li>(PMI(dog, bark) &#x3D; \log \frac{0.2326}{0.2326 \cdot 0.2326} &#x3D; \log \frac{0.2326}{0.0541} \approx \log 4.30 \approx 1.46)</li>
<li>(PPMI(dog, bark) &#x3D; \max(1.46, 0) &#x3D; 1.46)</li>
</ul>
</li>
</ol>
<p>以此类推，我们计算其他词对的PPMI值：</p>
<table>
<thead>
<tr>
<th></th>
<th>dog</th>
<th>cat</th>
<th>bird</th>
</tr>
</thead>
<tbody><tr>
<td>bark</td>
<td>1.46</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>run</td>
<td>0.58</td>
<td>0.58</td>
<td>0</td>
</tr>
<tr>
<td>fly</td>
<td>0</td>
<td>0</td>
<td>1.00</td>
</tr>
<tr>
<td>meow</td>
<td>0</td>
<td>1.00</td>
<td>0</td>
</tr>
<tr>
<td>chirp</td>
<td>0</td>
<td>0</td>
<td>1.00</td>
</tr>
</tbody></table>
<p>通过这种方式，我们对共现矩阵进行了熵归一化处理，减少了高频词对整体计算的影响，使得频率较高的元素对相似度计算的影响减小，从而更加准确地反映了词语之间的语义相关性。</p>
<h3 id="扩展–-双线形对数回归"><a href="#扩展–-双线形对数回归" class="headerlink" title="扩展– 双线形对数回归"></a>扩展– 双线形对数回归</h3><p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613210222568.png" alt="image-20240613210222568"></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240613210204537.png" alt="image-20240613210204537"></p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>引用自 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805">https://arxiv.org/pdf/1810.04805</a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul>
<li>Bert 的网络结构</li>
<li>与RNN和GPT的对比</li>
<li>自身的网路结构和参数</li>
</ul>
<h3 id="bert-的网路结构"><a href="#bert-的网路结构" class="headerlink" title="bert 的网路结构"></a>bert 的网路结构</h3><ol>
<li>输入表示（Input Representation）</li>
</ol>
<p>BERT的输入由三个嵌入（Embedding）相加而成：</p>
<ul>
<li><strong>Token Embeddings</strong>: 将每个词映射为一个固定维度的向量。</li>
<li><strong>Segment Embeddings</strong>: 区分句子A和句子B，用于句子对任务（如自然语言推断）。</li>
<li><strong>Position Embeddings</strong>: 表示词在序列中的位置，以捕捉词序信息。</li>
</ul>
<ol start="2">
<li>自注意力机制（Self-Attention Mechanism）</li>
</ol>
<p>自注意力机制是Transformer的核心。它通过计算输入序列中每个词对其他词的注意力分数来捕捉词与词之间的关系。具体步骤如下：</p>
<ol>
<li><p><strong>查询、键和值（Query, Key, Value）</strong>：</p>
<ul>
<li>对每个输入词，计算查询向量（Query）、键向量（Key）和值向量（Value）。</li>
<li>公式：[ Q &#x3D; XW^Q, K &#x3D; XW^K, V &#x3D; XW^V ]<br> 其中，( W^Q, W^K, W^V )是可训练的权重矩阵。</li>
</ul>
</li>
<li><p><strong>注意力分数（Attention Scores）</strong>：</p>
<ul>
<li>计算查询向量和键向量的点积，得到注意力分数，然后通过Softmax函数归一化。</li>
<li>公式：[ \text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V ]</li>
</ul>
</li>
<li><p><strong>多头注意力（Multi-Head Attention）</strong>：</p>
<ul>
<li>多头注意力机制将查询、键和值映射到多个子空间，并行计算多个注意力分数，然后将这些结果拼接起来。</li>
<li>公式：[ \text{MultiHead}(Q, K, V) &#x3D; \text{Concat}(\text{head}_1, \text{head}_2, …, \text{head}_h)W^O ]</li>
</ul>
</li>
<li><p>Transformer 编码器层（Transformer Encoder Layer）</p>
</li>
</ol>
<p>每个编码器层由两个主要部分组成：</p>
<ol>
<li><p><strong>多头自注意力层（Multi-Head Self-Attention Layer）</strong>：</p>
<ul>
<li>通过多头自注意力机制捕捉输入序列中不同位置的关系。</li>
</ul>
</li>
<li><p><strong>前馈神经网络（Feed-Forward Neural Network）</strong>：</p>
<ul>
<li>包含两个线性变换和一个ReLU激活函数。</li>
<li>公式：[ \text{FFN}(x) &#x3D; \text{max}(0, xW_1 + b_1)W_2 + b_2 ]</li>
</ul>
</li>
</ol>
<p>此外，每个编码器层还包括残差连接（Residual Connection）和层归一化（Layer Normalization）。</p>
<ol start="4">
<li>堆叠编码器层（Stacked Encoder Layers）</li>
</ol>
<p>BERT的网络结构通过堆叠多个编码器层构成。BERT base模型有12个编码器层，而BERT large模型有24个编码器层。</p>
<ol start="5">
<li>输出（Output）</li>
</ol>
<p>BERT的输出是每个词的上下文表示（Contextual Representation）。特别地，BERT使用[CLS]标记的表示作为整个输入序列的摘要，用于分类任务。</p>
<ol start="6">
<li>预训练任务（Pre-Training Tasks）</li>
</ol>
<p>BERT在预训练阶段主要使用了两个任务：</p>
<ol>
<li><p><strong>遮蔽语言模型（Masked Language Model, MLM）</strong>：</p>
<ul>
<li>随机遮蔽输入序列中的一些词，要求模型预测被遮蔽的词。</li>
</ul>
</li>
<li><p><strong>下一句预测（Next Sentence Prediction, NSP）</strong>：</p>
<ul>
<li>输入一对句子，要求模型预测第二个句子是否是第一个句子的下文。</li>
</ul>
</li>
</ol>
<p>BERT 网络结构图</p>
<p>以下是BERT的网络结构图示意图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Input Sequence (包含 [CLS], [SEP])</span><br><span class="line">    |</span><br><span class="line">    V</span><br><span class="line">Embedding Layer (Token Embeddings + Segment Embeddings + Position Embeddings)</span><br><span class="line">    |</span><br><span class="line">    V</span><br><span class="line">Stack of Encoder Layers (N Layers)</span><br><span class="line">    |</span><br><span class="line">    V</span><br><span class="line">[CLS] Token Representation (用于分类任务)</span><br><span class="line">    |</span><br><span class="line">    V</span><br><span class="line">其他Token Representations (用于其他任务)</span><br></pre></td></tr></table></figure>

<p>每个Encoder Layer的结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Input</span><br><span class="line">    |</span><br><span class="line">    V</span><br><span class="line">Multi-Head Self-Attention</span><br><span class="line">    |</span><br><span class="line">    +--&gt; Add &amp; Norm</span><br><span class="line">    |</span><br><span class="line">Feed-Forward Network</span><br><span class="line">    |</span><br><span class="line">    +--&gt; Add &amp; Norm</span><br><span class="line">    |</span><br><span class="line">Output</span><br></pre></td></tr></table></figure>

<p>通过上述结构，BERT能够对输入序列进行深层次的语义建模，捕捉词与词之间的复杂关系，从而在各种自然语言处理任务中取得优异的表现。</p>
<h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614204612026.png" alt="image-20240614204612026"></p>
<p>Transformer 核心网络架构</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614202023757.png" alt="image-20240614202023757"></p>
<h3 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h3><p>In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERT_BASE (L&#x3D;12, H&#x3D;768, A&#x3D;12, Total Parameters&#x3D;110M) and BERT_LARGE (L&#x3D;24, H&#x3D;1024, A&#x3D;16, Total Parameters&#x3D;340M).</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614194336699.png" alt="image-20240614194336699"></p>
<h3 id="pre-training-and-fine-tuning"><a href="#pre-training-and-fine-tuning" class="headerlink" title="pre-training and fine tuning"></a>pre-training and fine tuning</h3><p>We use WordPiece embeddings (Wu et al., 2016) with a &#x3D;&#x3D;30,000&#x3D;&#x3D; token vocabulary. The first token of every sequence is always a special classification token &#x3D;&#x3D;([CLS])&#x3D;&#x3D;. The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C ∈ RH, and the final hidden vector for the ith input token as Ti ∈ RH.</p>
<p>为什么不采用空格的方式进行分词？</p>
<p>pretrain 概率小  找其子序列 –&gt; pre </p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614195527537.png" alt="image-20240614195527537"></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240614200620628.png" alt="image-20240614200620628"></p>
<h3 id="pretrain"><a href="#pretrain" class="headerlink" title="pretrain"></a>pretrain</h3><ul>
<li>mask</li>
<li>NSP</li>
<li>pre- train data</li>
</ul>
<p>MASK</p>
<p>Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.</p>
<p>Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by</p>
<ul>
<li>80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy → my dog is &#x3D;&#x3D;[MASK]&#x3D;&#x3D;</li>
<li>10% of the time: Replace the word with a random word, e.g., my dog is hairy → my dog is &#x3D;&#x3D;apple&#x3D;&#x3D;</li>
<li>10% of the time: Keep the word unchanged, e.g., my dog is hairy → my dog is &#x3D;&#x3D;hairy&#x3D;&#x3D;. The purpose of this is to bias the representation towards the actual observed word.</li>
</ul>
<p>NSP</p>
<p>Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples.</p>
<p>Input &#x3D; [CLS] the man went to [MASK] store [SEP]<br>        he bought a gallon [MASK] milk [SEP]<br>Label &#x3D; IsNext</p>
<p>Input &#x3D; [CLS] the man [MASK] to the store [SEP]<br>        penguin [MASK] are &#x3D;&#x3D;flight ##less&#x3D;&#x3D; birds [SEP]<br>Label &#x3D; NotNext</p>
<p>Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP). Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI.</p>
<h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine tuning"></a>Fine tuning</h3><ul>
<li>与transformer 之间的 差异</li>
<li>simply plug in the task-specific inputs and outputs to Bert and fine tuning all the parameters end2end</li>
<li>GLUE （CLS + W 做多分类任务）</li>
<li>QA （阅读理解 – &gt; S and E ）</li>
</ul>
<h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><ul>
<li>effects of parts of model</li>
<li>Model size</li>
<li>fine- tuning</li>
</ul>
<p>参考和引用：</p>
<ul>
<li>技术论文原文</li>
<li>B站“跟李沐学AI” 、《动手学习深度学习》</li>
</ul>
<h2 id="任务一-基于方面情感分析的背景表示"><a href="#任务一-基于方面情感分析的背景表示" class="headerlink" title="任务一 基于方面情感分析的背景表示"></a>任务一 基于方面情感分析的背景表示</h2><h3 id="数据集参考"><a href="#数据集参考" class="headerlink" title="数据集参考"></a>数据集参考</h3><p>使用的数据集是 <a target="_blank" rel="noopener" href="https://github.com/xuuuluuu/SemEval-Triplet-data/tree/master/ASTE-Data-V2-EMNLP2020">https://github.com/xuuuluuu/SemEval-Triplet-data/tree/master/ASTE-Data-V2-EMNLP2020</a></p>
<h3 id="使用bert-背景表示"><a href="#使用bert-背景表示" class="headerlink" title="使用bert- 背景表示"></a>使用bert- 背景表示</h3><p>bert 是如何将一个句子转为向量的？</p>
<p>BERT（Bidirectional Encoder Representations from Transformers）将一句话转换为向量的过程涉及多个步骤，包括分词、添加特殊标记、生成输入嵌入、通过Transformer编码器处理等。以下是详细步骤：</p>
<ol>
<li>分词（Tokenization）</li>
</ol>
<p>首先，BERT使用其专用的分词器（如<code>BertTokenizer</code>）将输入的句子转换为一系列子词（subword）单元。BERT采用WordPiece分词算法，它将词分解为更小的单元，使得词汇量能够有效地覆盖大量单词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">sentence = <span class="string">&quot;This is an example sentence.&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(sentence)</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure>

<p>输出可能是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;this&#x27;, &#x27;is&#x27;, &#x27;an&#x27;, &#x27;example&#x27;, &#x27;sentence&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>添加特殊标记（Special Tokens）</li>
</ol>
<p>BERT需要在输入序列的开头添加<code>[CLS]</code>标记，表示分类任务的起点，在每个句子（或句子对）之间添加<code>[SEP]</code>标记。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokens = [<span class="string">&#x27;[CLS]&#x27;</span>] + tokens + [<span class="string">&#x27;[SEP]&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure>

<p>输出可能是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;[CLS]&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;an&#x27;, &#x27;example&#x27;, &#x27;sentence&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>生成输入嵌入（Input Embeddings）</li>
</ol>
<p>分词后，BERT将每个子词单元转换为相应的词嵌入。这些嵌入由以下几部分组成：</p>
<ul>
<li><strong>Token Embeddings</strong>：分词器将每个子词映射到一个固定大小的向量（通常是768维）。</li>
<li><strong>Segment Embeddings</strong>：用于表示句子对中的句子A和句子B。</li>
<li><strong>Position Embeddings</strong>：用于表示每个子词在句子中的位置。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line">segment_ids = [<span class="number">0</span>] * <span class="built_in">len</span>(tokens)  <span class="comment"># 单句子任务中，所有的segment id都为0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将这些ID转换为张量</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">input_ids = torch.tensor([input_ids])</span><br><span class="line">segment_ids = torch.tensor([segment_ids])</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>通过Transformer编码器（Transformer Encoder）</li>
</ol>
<p>BERT模型的核心是多层Transformer编码器。输入的词嵌入通过多层双向Transformer编码器进行处理。每一层包含多个自注意力头（self-attention heads），它们可以捕捉到句子中不同位置的单词之间的依赖关系。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">outputs = model(input_ids, token_type_ids=segment_ids)</span><br><span class="line">last_hidden_states = outputs.last_hidden_state</span><br></pre></td></tr></table></figure>

<ul>
<li><code>input_ids</code>：表示输入句子的token ids。</li>
<li><code>segment_ids</code>：表示句子对中句子的分段id（对于单句任务，所有值都为0）。</li>
<li><code>last_hidden_states</code>：模型的最后一个隐藏层的输出，是每个token的表示向量。</li>
</ul>
<ol start="5">
<li>获取句子向量（Sentence Embedding）</li>
</ol>
<p>对于句子分类任务，通常使用<code>[CLS]</code>标记对应的向量作为整个句子的表示向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cls_embedding = last_hidden_states[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># [CLS]标记的向量</span></span><br><span class="line"><span class="built_in">print</span>(cls_embedding)</span><br></pre></td></tr></table></figure>

<p><code>cls_embedding</code>就是句子的表示向量，可以用于下游任务如分类、相似度计算等。</p>
<h4 id="嵌入总结"><a href="#嵌入总结" class="headerlink" title="嵌入总结"></a>嵌入总结</h4><ul>
<li><strong>分词</strong>：将输入句子分解为子词单元。</li>
<li><strong>添加特殊标记</strong>：在序列前后添加<code>[CLS]</code>和<code>[SEP]</code>标记。</li>
<li><strong>生成输入嵌入</strong>：将子词单元转换为词嵌入，并加上段嵌入和位置嵌入。</li>
<li><strong>通过Transformer编码器</strong>：通过多层双向Transformer编码器处理嵌入。</li>
<li><strong>获取句子向量</strong>：通常使用<code>[CLS]</code>标记对应的向量作为句子的表示向量。</li>
</ul>
<p>这种方式使得BERT可以有效地捕捉到输入句子的语义信息，并将其转换为固定大小的向量表示，便于进行各种自然语言处理任务。</p>
<p>代码测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化BERT分词器和模型</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>, output_hidden_states=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例句子</span></span><br><span class="line">sentence = <span class="string">&quot;BERT is a powerful language model.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对句子进行分词</span></span><br><span class="line">inputs = tokenizer(sentence, return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="string">&#x27;max_length&#x27;</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分词后的输入传递给模型</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取[CLS]标记的向量表示</span></span><br><span class="line">cls_embedding = outputs.hidden_states[-<span class="number">1</span>][:, <span class="number">0</span>, :]  <span class="comment"># 最后一层的[CLS]标记向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印[CLS]标记的向量表示</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CLS embedding for the sentence:&quot;</span>, cls_embedding[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>





<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615191139130.png" alt="image-20240615191139130"></p>
<h3 id="使用-gloVE-–-背景表示"><a href="#使用-gloVE-–-背景表示" class="headerlink" title="使用 gloVE – 背景表示"></a>使用 gloVE – 背景表示</h3><p>gloVe 是如何 将单词转换为 向量的 ？</p>
<p>简单的代码测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> GloVe</span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的GloVe词向量</span></span><br><span class="line">glove = GloVe(name=<span class="string">&#x27;6B&#x27;</span>, dim=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建单词到索引的映射</span></span><br><span class="line">vocab = &#123;word: idx <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(glove.itos)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义分词器</span></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&quot;basic_english&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例句子</span></span><br><span class="line">sentence = <span class="string">&quot;This is an example sentence.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将句子分词并转换为GloVe词向量</span></span><br><span class="line">tokens = tokenizer(sentence)</span><br><span class="line">indices = [vocab[token] <span class="keyword">if</span> token <span class="keyword">in</span> vocab <span class="keyword">else</span> vocab[<span class="string">&#x27;unk&#x27;</span>] <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">vectors = [glove.vectors[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tokens:&quot;</span>, tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Indices:&quot;</span>, indices)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Vectors:&quot;</span>, vectors)</span><br></pre></td></tr></table></figure>



<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615183425785.png" alt="image-20240615183425785"></p>
<h3 id="GloVe-vs-BERT"><a href="#GloVe-vs-BERT" class="headerlink" title="GloVe vs. BERT"></a><strong>GloVe vs. BERT</strong></h3><p><strong>GloVe</strong>：</p>
<p>​		GloVe（Global Vectors for Word Representation）是静态的词向量，它为每个词提供一个固定的向量表示。这意味着一个词在所有上下文中都具有相同的向量表示。虽然GloVe捕捉到了全局的词语共现信息，但它不能处理单词在不同上下文中的不同含义。</p>
<p><strong>BERT</strong>：</p>
<p>​		BERT（Bidirectional Encoder Representations from Transformers）是基于Transformer的模型，能够捕捉到上下文依赖。它通过双向编码器（同时从左到右和从右到左）来生成词的动态表示。每个词的向量表示是根据其在特定上下文中的位置来确定的。BERT不仅能处理单词之间的上下文依赖，还能处理句子之间的上下文依赖。</p>
<p>Bert 测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化分词器和模型</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>, output_hidden_states=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例句子</span></span><br><span class="line">sentences = [<span class="string">&quot;He went to the bank to deposit money.&quot;</span>, <span class="string">&quot;The river bank was covered in snow.&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取[CLS]标记的向量表示</span></span><br><span class="line">    cls_embedding = outputs.hidden_states[-<span class="number">1</span>][:, <span class="number">0</span>, :]  <span class="comment"># 最后一层的[CLS]标记向量</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence: <span class="subst">&#123;sentence&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;CLS embedding: <span class="subst">&#123;cls_embedding[<span class="number">0</span>][:<span class="number">5</span>]&#125;</span>&quot;</span>)  <span class="comment"># 只打印前5维向量以简化输出</span></span><br></pre></td></tr></table></figure>

<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615193424977.png" alt="image-20240615193424977"></p>
<p>gloVe测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载GloVe词向量</span></span><br><span class="line">embedding_index = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/Users/bootscoder/PycharmProjects/nlp-lesson-design/test1-sentiment/.vector_cache/glove.6B.100d.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        values = line.split()</span><br><span class="line">        word = values[<span class="number">0</span>]</span><br><span class="line">        coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">        embedding_index[word] = coefs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数获取GloVe词向量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_glove_embedding</span>(<span class="params">word</span>):</span><br><span class="line">    <span class="keyword">return</span> embedding_index.get(word, np.zeros(<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例句子</span></span><br><span class="line">sentence1 = <span class="string">&quot;He went to the bank to deposit money.&quot;</span></span><br><span class="line">sentence2 = <span class="string">&quot;The river bank was covered in snow.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取“bank”的词向量</span></span><br><span class="line">bank_embedding1 = get_glove_embedding(<span class="string">&quot;bank&quot;</span>)</span><br><span class="line">bank_embedding2 = get_glove_embedding(<span class="string">&quot;bank&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;GloVe embedding for &#x27;bank&#x27; in sentence 1:&quot;</span>, bank_embedding1[:<span class="number">5</span>])  <span class="comment"># 打印前5维以简化输出</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;GloVe embedding for &#x27;bank&#x27; in sentence 2:&quot;</span>, bank_embedding2[:<span class="number">5</span>])  <span class="comment"># 打印前5维以简化输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较两个向量是否相同</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Are the embeddings the same?&quot;</span>, np.array_equal(bank_embedding1, bank_embedding2))</span><br></pre></td></tr></table></figure>



<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615193404410.png" alt="image-20240615193404410"></p>
<h2 id="任务二-基于方面情感分析任务的句法特征表示"><a href="#任务二-基于方面情感分析任务的句法特征表示" class="headerlink" title="任务二 基于方面情感分析任务的句法特征表示"></a>任务二 基于方面情感分析任务的句法特征表示</h2><p>参考：</p>
<p>教学ppt ：  <a target="_blank" rel="noopener" href="http://www.nlpir.org/wordpress/wp-content/uploads/2022/04/%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%902022.pdf">http://www.nlpir.org/wordpress/wp-content/uploads/2022/04/%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%902022.pdf</a></p>
<p>哈工大资源： <a target="_blank" rel="noopener" href="https://ltp.readthedocs.io/zh-cn/latest/">https://ltp.readthedocs.io/zh-cn/latest/</a></p>
<h3 id="tool"><a href="#tool" class="headerlink" title="tool"></a>tool</h3><p>​	1.	**自然语言处理工具包 (NLP Toolkits)**：</p>
<p>​	•	<strong>Stanford NLP</strong>：斯坦福大学开发的自然语言处理工具包，提供了强大的句法分析器。可以对句子进行词性标注、依存分析等。</p>
<p>​	•	<strong>spaCy</strong>：一个高效且易用的NLP库，支持多种语言的句法分析。它提供了预训练的模型，可以快速进行句法树解析。</p>
<p>​	•	**NLTK (Natural Language Toolkit)**：一个老牌的NLP工具包，虽然性能不如前两个，但提供了很多教学资源和句法分析的基础工具。</p>
<p>​	2.	<strong>预训练模型和API</strong>：</p>
<p>​	•	<strong>AllenNLP</strong>：一个基于PyTorch的NLP研究库，提供了预训练的句法分析模型。</p>
<p>​	•	<strong>Google Cloud Natural Language API</strong>：谷歌提供的云服务，支持句法分析，可以直接调用API进行句法树解析。</p>
<p>​	3.	<strong>依存分析（Dependency Parsing）</strong>：</p>
<p>​	•	依存分析可以解析句子的依存关系，识别句子中词语之间的语法依赖关系。可以使用Stanford NLP或spaCy进行依存分析。</p>
<p>​	4.	<strong>成分分析（Constituency Parsing）</strong>：</p>
<p>​	•	成分分析将句子分解成短语结构树，标识句子的短语结构。可以使用Stanford NLP进行成分分析。</p>
<p>​	5.	<strong>树形可视化</strong>：</p>
<p>​	•	句法树的可视化有助于理解句子的结构。可以使用Matplotlib结合NLTK或spaCy生成句法树的可视化图。</p>
<h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy <span class="keyword">import</span> displacy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的英语模型</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入句子</span></span><br><span class="line">sentence = <span class="string">&quot;The quick brown fox jumps over the lazy dog.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行句法分析</span></span><br><span class="line">doc = nlp(sentence)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示依存关系</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> doc:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;token.text&#125;</span> -&gt; <span class="subst">&#123;token.dep_&#125;</span> -&gt; <span class="subst">&#123;token.head.text&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化句法树，设置图形大小</span></span><br><span class="line">options = &#123;<span class="string">&quot;compact&quot;</span>: <span class="literal">True</span>, <span class="string">&quot;bg&quot;</span>: <span class="string">&quot;#fafafa&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;#000000&quot;</span>, <span class="string">&quot;font&quot;</span>: <span class="string">&quot;Source Sans Pro&quot;</span>, <span class="string">&quot;distance&quot;</span>: <span class="number">90</span>&#125;</span><br><span class="line"></span><br><span class="line">displacy.render(doc, style=<span class="string">&quot;dep&quot;</span>, jupyter=<span class="literal">True</span>, options=options)</span><br></pre></td></tr></table></figure>



<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615202736646.png" alt="image-20240615202736646"></p>
<p>​	1.	<strong>依存关系（Dependency Relations）</strong>:</p>
<p>​	•	The (DET) -&gt; fox (NOUN) 表示定冠词 “The” 修饰名词 “fox”。</p>
<p>​	•	quick (ADJ) -&gt; fox (NOUN) 表示形容词 “quick” 修饰名词 “fox”。</p>
<p>​	•	brown (ADJ) -&gt; fox (NOUN) 表示形容词 “brown” 修饰名词 “fox”。</p>
<p>​	•	fox (NOUN) -&gt; jumps (VERB) 表示名词 “fox” 是动词 “jumps” 的主语。</p>
<p>​	•	jumps (VERB) -&gt; ROOT 表示 “jumps” 是句子的谓语动词，作为句子的根。</p>
<p>​	•	over (ADP) -&gt; jumps (VERB) 表示介词 “over” 修饰动词 “jumps”。</p>
<p>​	•	the (DET) -&gt; dog (NOUN) 表示定冠词 “the” 修饰名词 “dog”。</p>
<p>​	•	lazy (ADJ) -&gt; dog (NOUN) 表示形容词 “lazy” 修饰名词 “dog”。</p>
<p>​	•	dog (NOUN) -&gt; over (ADP) 表示名词 “dog” 是介词 “over” 的宾语。</p>
<p>​	•	. (PUNCT) -&gt; jumps (VERB) 表示句子末尾的句号。</p>
<p>​	2.	<strong>图形显示</strong>:</p>
<p>​	•	图中每个词的上方显示了依存关系标签（如 det、amod、nsubj 等），这些标签描述了词与词之间的语法关系。</p>
<p>​	•	图中箭头表示依存方向，即从修饰词指向被修饰词。例如，quick 指向 fox，表示 quick 修饰 fox。</p>
<p>​	•	每个词下方显示了词性标签（如 DET、ADJ、NOUN、VERB、ADP、PUNCT 等），这些标签描述了词的词性。</p>
<h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>​	1.	<strong>理解句子的结构</strong>：通过句法分析，可以识别句子的组成部分，如主语、谓语、宾语等，了解它们之间的关系。</p>
<p>​	2.	<strong>语义理解的基础</strong>：句法分析为更深层次的语义分析提供了基础，因为只有明确了句子的结构，才能准确理解句子的含义。</p>
<p>​	3.	<strong>改进机器翻译和问答系统</strong>：在机器翻译和自动问答系统中，句法分析有助于系统理解和生成更加准确、流畅的语言。</p>
<p>​	4.	<strong>信息抽取</strong>：通过句法分析，可以从文本中抽取出有用的信息，如实体、关系和事件等。</p>
<p>​	5.	<strong>提升自然语言生成</strong>：在自然语言生成任务中，句法分析可以帮助生成符合语法规则的句子，使生成的文本更加自然。</p>
<h2 id="任务三-基于方面情感–背景表示和句法特征表示的融合"><a href="#任务三-基于方面情感–背景表示和句法特征表示的融合" class="headerlink" title="任务三 基于方面情感–背景表示和句法特征表示的融合"></a>任务三 基于方面情感–背景表示和句法特征表示的融合</h2><h3 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h3><p>请你设计一个 demo 使用 gloVe做词嵌入， 使用spacy得到句法特征， 想办法结合二者来做一个情感分类任务 ，情感的label 有 积极 消极 中性</p>
<h3 id="融合方法"><a href="#融合方法" class="headerlink" title="融合方法"></a>融合方法</h3><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>句法特征和词嵌入做简单的 一维度拼接 – </p>
<ul>
<li>我们可以将句法特征的大小固定为一个特定的长度，例如：选择前10个词的句法特征，如果不足10个词则补零。</li>
</ul>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616010054527.png" alt="image-20240616010054527"></p>
<h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><p>找到句法中的中心词增加分类时候的权重 — 在本例中为形容词</p>
<h3 id="demo-1"><a href="#demo-1" class="headerlink" title="demo"></a>demo</h3><p>随机生成一个包含10条文本和情感标签（积极、消极、中性）的数据集，并使用这个数据集来演示整个流程。</p>
<p>步骤 1: 安装所需库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install spacy numpy pandas scikit-learn</span><br><span class="line">python -m spacy download en_core_web_sm</span><br></pre></td></tr></table></figure>

<p>步骤 2: 生成示例数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成示例数据集</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;text&#x27;</span>: [</span><br><span class="line">        <span class="string">&#x27;I love this product, it is amazing!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;This is the worst service I have ever received.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The movie was okay, not great but not bad either.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I am extremely happy with the results!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The food was terrible and the wait was too long.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;It was an average experience, nothing special.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I absolutely adore this place!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I hate the way they treat their customers.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The book was decent, a good read overall.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I feel indifferent about this situation.&#x27;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&#x27;label&#x27;</span>: [<span class="string">&#x27;positive&#x27;</span>, <span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;positive&#x27;</span>, <span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;positive&#x27;</span>, <span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df.to_csv(<span class="string">&#x27;demo_dataset.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>步骤 3: 加载GloVe词嵌入和数据</p>
<p>我们需要一个预训练的GloVe词嵌入文件，你可以从<a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">这里</a>下载并放在合适的路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载GloVe词嵌入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_glove_embeddings</span>(<span class="params">file_path</span>):</span><br><span class="line">    embeddings_index = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            values = line.split()</span><br><span class="line">            word = values[<span class="number">0</span>]</span><br><span class="line">            coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            embeddings_index[word] = coefs</span><br><span class="line">    <span class="keyword">return</span> embeddings_index</span><br><span class="line"></span><br><span class="line">glove_file = <span class="string">&#x27;path_to_glove_file.txt&#x27;</span>  <span class="comment"># 替换为你的GloVe文件路径</span></span><br><span class="line">embeddings_index = load_glove_embeddings(glove_file)</span><br></pre></td></tr></table></figure>

<p>步骤 4: 加载和预处理数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载SpaCy模型</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;demo_dataset.csv&#x27;</span>)</span><br><span class="line">texts = data[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">labels = data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转换为数字</span></span><br><span class="line">label_mapping = &#123;<span class="string">&#x27;positive&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;negative&#x27;</span>: -<span class="number">1</span>, <span class="string">&#x27;neutral&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">labels = labels.<span class="built_in">map</span>(label_mapping)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取GloVe词嵌入和句法特征</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_features</span>(<span class="params">text, embeddings_index</span>):</span><br><span class="line">    doc = nlp(text)</span><br><span class="line">    embedding_features = np.mean([embeddings_index.get(token.text, np.zeros((<span class="number">300</span>,))) <span class="keyword">for</span> token <span class="keyword">in</span> doc], axis=<span class="number">0</span>)</span><br><span class="line">    syntax_features = np.array([token.dep_ <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line">    <span class="comment"># 将句法特征转换为数值特征</span></span><br><span class="line">    syntax_features = np.array([<span class="built_in">hash</span>(feature) % <span class="number">1000</span> <span class="keyword">for</span> feature <span class="keyword">in</span> syntax_features])  <span class="comment"># 简单哈希处理</span></span><br><span class="line">    <span class="keyword">return</span> np.concatenate((embedding_features, syntax_features))</span><br><span class="line"></span><br><span class="line">features = np.array([get_features(text, embeddings_index) <span class="keyword">for</span> text <span class="keyword">in</span> texts])</span><br><span class="line"></span><br><span class="line">V2 版本 可对比</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取GloVe词嵌入和句法特征</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_features</span>(<span class="params">text, embeddings_index</span>):</span><br><span class="line">    doc = nlp(text)</span><br><span class="line">    embedding_features = np.mean([embeddings_index.get(token.text, np.zeros((<span class="number">100</span>,))) <span class="keyword">for</span> token <span class="keyword">in</span> doc], axis=<span class="number">0</span>)</span><br><span class="line">    syntax_features = np.array([token.dep_ <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line">    <span class="comment"># 将句法特征转换为数值特征</span></span><br><span class="line">    syntax_features = np.array([<span class="built_in">hash</span>(feature) % <span class="number">1000</span> <span class="keyword">for</span> feature <span class="keyword">in</span> syntax_features])</span><br><span class="line">    <span class="comment"># 将句法特征填充到固定长度</span></span><br><span class="line">    max_length = <span class="number">10</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(syntax_features) &gt; max_length):</span><br><span class="line">        syntax_features = syntax_features[:max_length]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        syntax_features = np.pad(syntax_features, (<span class="number">0</span>, max_length - <span class="built_in">len</span>(syntax_features)), <span class="string">&#x27;constant&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> np.concatenate((embedding_features, syntax_features))</span><br></pre></td></tr></table></figure>

<p>步骤 5: 训练和评估模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测和评估</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=[<span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;positive&#x27;</span>]))</span><br></pre></td></tr></table></figure>





<h4 id="完整代码：-使用-random-forest"><a href="#完整代码：-使用-random-forest" class="headerlink" title="完整代码： 使用 random forest"></a>完整代码： 使用 random forest</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成示例数据集</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;text&#x27;</span>: [</span><br><span class="line">        <span class="string">&#x27;I love this product, it is amazing!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;This is the worst service I have ever received.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The movie was okay, not great but not bad either.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I am extremely happy with the results!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The food was terrible and the wait was too long.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;It was an average experience, nothing special.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I absolutely adore this place!&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I hate the way they treat their customers.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The book was decent, a good read overall.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;I feel indifferent about this situation.&#x27;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&#x27;label&#x27;</span>: [<span class="string">&#x27;positive&#x27;</span>, <span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;positive&#x27;</span>, <span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;positive&#x27;</span>, <span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df.to_csv(<span class="string">&#x27;demo_dataset.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载GloVe词嵌入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_glove_embeddings</span>(<span class="params">file_path</span>):</span><br><span class="line">    embeddings_index = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            values = line.split()</span><br><span class="line">            word = values[<span class="number">0</span>]</span><br><span class="line">            coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            embeddings_index[word] = coefs</span><br><span class="line">    <span class="keyword">return</span> embeddings_index</span><br><span class="line"></span><br><span class="line">glove_file = <span class="string">&#x27;/Users/bootscoder/PycharmProjects/nlp-lesson-design/test3/.vector_cache/glove.6B.100d.txt&#x27;</span>  <span class="comment"># 替换为你的GloVe文件路径</span></span><br><span class="line">embeddings_index = load_glove_embeddings(glove_file)</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载SpaCy模型</span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;demo_dataset.csv&#x27;</span>)</span><br><span class="line">texts = data[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">labels = data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转换为数字</span></span><br><span class="line">label_mapping = &#123;<span class="string">&#x27;positive&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;negative&#x27;</span>: -<span class="number">1</span>, <span class="string">&#x27;neutral&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">labels = labels.<span class="built_in">map</span>(label_mapping)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取GloVe词嵌入和句法特征</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_features</span>(<span class="params">text, embeddings_index</span>):</span><br><span class="line">    doc = nlp(text)</span><br><span class="line">    embedding_features = np.mean([embeddings_index.get(token.text, np.zeros((<span class="number">100</span>,))) <span class="keyword">for</span> token <span class="keyword">in</span> doc], axis=<span class="number">0</span>)</span><br><span class="line">    syntax_features = np.array([token.dep_ <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br><span class="line">    <span class="comment"># 将句法特征转换为数值特征</span></span><br><span class="line">    syntax_features = np.array([<span class="built_in">hash</span>(feature) % <span class="number">1000</span> <span class="keyword">for</span> feature <span class="keyword">in</span> syntax_features])</span><br><span class="line">    <span class="comment"># 将句法特征填充到固定长度</span></span><br><span class="line">    max_length = <span class="number">10</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(syntax_features) &gt; max_length):</span><br><span class="line">        syntax_features = syntax_features[:max_length]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        syntax_features = np.pad(syntax_features, (<span class="number">0</span>, max_length - <span class="built_in">len</span>(syntax_features)), <span class="string">&#x27;constant&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> np.concatenate((embedding_features, syntax_features))</span><br><span class="line"></span><br><span class="line">features = np.array([get_features(text, embeddings_index) <span class="keyword">for</span> text <span class="keyword">in</span> texts])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测和评估</span></span><br><span class="line">y_pred = model.predict(X_train)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_train, y_pred, target_names=[<span class="string">&#x27;negative&#x27;</span>, <span class="string">&#x27;neutral&#x27;</span>, <span class="string">&#x27;positive&#x27;</span>]))</span><br></pre></td></tr></table></figure>



<h4 id="完整代码使用-bi-lstm-attention"><a href="#完整代码使用-bi-lstm-attention" class="headerlink" title="完整代码使用 bi -lstm + attention"></a>完整代码使用 bi -lstm + attention</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Layer</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载GloVe词嵌入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_glove_embeddings</span>(<span class="params">file_path</span>):</span><br><span class="line">    embeddings_index = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            values = line.split()</span><br><span class="line">            word = values[<span class="number">0</span>]</span><br><span class="line">            coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            embeddings_index[word] = coefs</span><br><span class="line">    <span class="keyword">return</span> embeddings_index</span><br><span class="line"></span><br><span class="line">glove_file = <span class="string">&#x27;/Users/bootscoder/PycharmProjects/nlp-lesson-design/test3/.vector_cache/glove.6B.100d.txt&#x27;</span>  <span class="comment"># 替换为你的GloVe文件路径</span></span><br><span class="line">embeddings_index = load_glove_embeddings(glove_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;demo_dataset.csv&#x27;</span>)</span><br><span class="line">texts = data[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">labels = data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将标签转换为数字</span></span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">labels = label_encoder.fit_transform(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Tokenizer对文本进行编码</span></span><br><span class="line">tokenizer = Tokenizer()</span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充序列</span></span><br><span class="line">max_sequence_length = <span class="number">100</span></span><br><span class="line">data = pad_sequences(sequences, maxlen=max_sequence_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建词嵌入矩阵</span></span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">embedding_dim = <span class="number">100</span></span><br><span class="line">embedding_matrix = np.zeros((<span class="built_in">len</span>(word_index) + <span class="number">1</span>, embedding_dim))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">    embedding_vector = embeddings_index.get(word)</span><br><span class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        embedding_matrix[i] = embedding_vector</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义注意力层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionLayer</span>(<span class="title class_ inherited__">Layer</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        self.W = self.add_weight(name=<span class="string">&#x27;att_weight&#x27;</span>, shape=(input_shape[-<span class="number">1</span>], <span class="number">1</span>), initializer=<span class="string">&#x27;uniform&#x27;</span>, trainable=<span class="literal">True</span>)</span><br><span class="line">        self.b = self.add_weight(name=<span class="string">&#x27;att_bias&#x27;</span>, shape=(input_shape[<span class="number">1</span>], <span class="number">1</span>), initializer=<span class="string">&#x27;uniform&#x27;</span>, trainable=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">super</span>(AttentionLayer, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        e = K.tanh(K.dot(x, self.W) + self.b)</span><br><span class="line">        e = K.squeeze(e, axis=-<span class="number">1</span>)</span><br><span class="line">        alpha = K.softmax(e)</span><br><span class="line">        alpha = K.expand_dims(alpha, axis=-<span class="number">1</span>)</span><br><span class="line">        context = x * alpha</span><br><span class="line">        context = K.<span class="built_in">sum</span>(context, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> context</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_output_shape</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        <span class="keyword">return</span> (input_shape[<span class="number">0</span>], input_shape[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建带有注意力机制的LSTM模型</span></span><br><span class="line"><span class="built_in">input</span> = Input(shape=(max_sequence_length,))</span><br><span class="line">embedding = Embedding(<span class="built_in">len</span>(word_index) + <span class="number">1</span>, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=<span class="literal">False</span>)(<span class="built_in">input</span>)</span><br><span class="line">x = Bidirectional(LSTM(<span class="number">100</span>, return_sequences=<span class="literal">True</span>))(embedding)</span><br><span class="line">x = AttentionLayer()(x)</span><br><span class="line">x = Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">output = Dense(<span class="number">3</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(inputs=<span class="built_in">input</span>, outputs=output)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(data, labels, epochs=<span class="number">20</span>, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉验证数据集 ，容易使得 model 学到歪门邪道 -- 数据量少</span></span><br><span class="line"><span class="comment"># model.fit(data, labels, epochs=20, batch_size=32, validatation_slpit = 0.2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测和评估</span></span><br><span class="line">predictions = model.predict(data)</span><br><span class="line">y_pred = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(labels, y_pred, target_names=label_encoder.classes_))</span><br></pre></td></tr></table></figure>



<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616012907074.png" alt="image-20240616012907074"></p>
<p>感觉应该是收到 gloVe的影响了</p>
<p>举例 good 和 great 非常接近</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616013045329.png" alt="image-20240616013045329"></p>
<h2 id="Task3-Bert"><a href="#Task3-Bert" class="headerlink" title="Task3 - Bert"></a>Task3 - Bert</h2><h3 id="正式处理数据集-–-bert"><a href="#正式处理数据集-–-bert" class="headerlink" title="正式处理数据集 – bert"></a>正式处理数据集 – bert</h3><p>​	1.	使用 spaCy 进行句法特征提取。</p>
<p>​	2.	将句法特征与 BERT 的词向量进行拼接。</p>
<p>​	3.	修改 SentimentDataset 类，使其支持句法特征的拼接。</p>
<p>​	4.	修改模型训练和评估的部分，使其支持新的输入。</p>
<p>数据预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line">data_path = <span class="string">&#x27;/Users/bootscoder/PycharmProjects/nlp-lesson-design/data/SemEval-Triplet-data/ASTE-Data-V2-EMNLP2020/14lap/dev_triplets.txt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">        text, label = line.strip().split(<span class="string">&#x27;####&#x27;</span>)</span><br><span class="line">        data.append((text, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为DataFrame</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单预处理标签</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	1.	如果label列中的值包含字符串&#x27;NEG&#x27;，则将该值转换为0。</span></span><br><span class="line"><span class="string">	2.	如果label列中的值包含字符串&#x27;NEU&#x27;，则将该值转换为1。</span></span><br><span class="line"><span class="string">	3.	如果label列中的值不包含&#x27;NEG&#x27;或&#x27;NEU&#x27;，则将该值转换为2。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">df[<span class="string">&#x27;label&#x27;</span>] = df[<span class="string">&#x27;label&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">0</span> <span class="keyword">if</span> <span class="string">&#x27;NEG&#x27;</span> <span class="keyword">in</span> x <span class="keyword">else</span> (<span class="number">1</span> <span class="keyword">if</span> <span class="string">&#x27;NEU&#x27;</span> <span class="keyword">in</span> x <span class="keyword">else</span> <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练和测试集</span></span><br><span class="line">train_df, test_df = train_test_split(df, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">train_texts = train_df[<span class="string">&#x27;sentence&#x27;</span>].tolist()</span><br><span class="line">train_labels = train_df[<span class="string">&#x27;label&#x27;</span>].tolist()</span><br><span class="line">test_texts = test_df[<span class="string">&#x27;sentence&#x27;</span>].tolist()</span><br><span class="line">test_labels = test_df[<span class="string">&#x27;label&#x27;</span>].tolist()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>将数据集进行转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, texts, labels, tokenizer, max_length</span>):</span><br><span class="line">        self.texts = texts</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        text = self.texts[idx]</span><br><span class="line">        label = self.labels[idx]</span><br><span class="line">        inputs = self.tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="string">&#x27;max_length&#x27;</span>, truncation=<span class="literal">True</span>, max_length=self.max_length)</span><br><span class="line">        inputs = &#123;key: val.squeeze(<span class="number">0</span>) <span class="keyword">for</span> key, val <span class="keyword">in</span> inputs.items()&#125;  <span class="comment"># Remove batch dimension</span></span><br><span class="line">        inputs[<span class="string">&#x27;labels&#x27;</span>] = torch.tensor(label, dtype=torch.long)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span></span><br></pre></td></tr></table></figure>



<p>数据预处理结果</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240615180535827.png" alt="image-20240615180535827"></p>
<p>完整代码</p>
<p>见笔者GitHub</p>
<h3 id="正式处理数据集-–-bert-spacy"><a href="#正式处理数据集-–-bert-spacy" class="headerlink" title="正式处理数据集 – bert+spacy"></a>正式处理数据集 – bert+spacy</h3><p>数据处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, texts, labels, tokenizer, max_length, nlp</span>):</span><br><span class="line">        self.texts = texts</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.max_length = max_length</span><br><span class="line">        self.nlp = nlp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        text = self.texts[idx]</span><br><span class="line">        label = self.labels[idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取句法特征</span></span><br><span class="line">        doc = self.nlp(text)</span><br><span class="line">        syntax_features = [token.dep_ <span class="keyword">for</span> token <span class="keyword">in</span> doc]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用BERT分词器对文本进行编码</span></span><br><span class="line">        inputs = self.tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="string">&#x27;max_length&#x27;</span>, truncation=<span class="literal">True</span>,</span><br><span class="line">                                max_length=self.max_length)</span><br><span class="line">        inputs = &#123;key: val.squeeze(<span class="number">0</span>) <span class="keyword">for</span> key, val <span class="keyword">in</span> inputs.items()&#125;  <span class="comment"># Remove batch dimension</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化句法特征张量</span></span><br><span class="line">        syntax_tensor = torch.zeros(self.max_length, <span class="built_in">len</span>(self.nlp.get_pipe(<span class="string">&quot;parser&quot;</span>).labels))</span><br><span class="line">        <span class="keyword">for</span> i, dep <span class="keyword">in</span> <span class="built_in">enumerate</span>(syntax_features[:self.max_length]):</span><br><span class="line">            <span class="keyword">if</span> dep <span class="keyword">in</span> self.nlp.get_pipe(<span class="string">&quot;parser&quot;</span>).labels:</span><br><span class="line">                syntax_tensor[i][self.nlp.get_pipe(<span class="string">&quot;parser&quot;</span>).labels.index(dep)] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算句法特征的平均值</span></span><br><span class="line">        syntax_features_avg = torch.mean(syntax_tensor, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将句法特征张量添加到inputs字典中</span></span><br><span class="line">        inputs[<span class="string">&#x27;syntax&#x27;</span>] = syntax_features_avg</span><br><span class="line">        inputs[<span class="string">&#x27;labels&#x27;</span>] = torch.tensor(label, dtype=torch.long)</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>自定义bert模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义自定义BERT模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomBERTModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_labels, syntax_feature_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(CustomBERTModel, self).__init__()</span><br><span class="line">        config = BertConfig.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">        config.output_hidden_states = <span class="literal">True</span>  <span class="comment"># 启用hidden states输出</span></span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>, config=config)</span><br><span class="line">        self.classifier = torch.nn.Linear(config.hidden_size + syntax_feature_size, num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, token_type_ids=<span class="literal">None</span>, syntax=<span class="literal">None</span></span>):</span><br><span class="line">        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)</span><br><span class="line">        cls_output = outputs.last_hidden_state[:, <span class="number">0</span>, :]  <span class="comment"># [CLS] token output</span></span><br><span class="line">        combined_output = torch.cat((cls_output, syntax), dim=<span class="number">1</span>)  <span class="comment"># Combine with syntax features</span></span><br><span class="line">        logits = self.classifier(combined_output)</span><br><span class="line">        <span class="keyword">return</span> logits, outputs.hidden_states</span><br></pre></td></tr></table></figure>



<p>完整代码见笔者 GitHub</p>
<h3 id="bert-VS-bert-spacy"><a href="#bert-VS-bert-spacy" class="headerlink" title="bert VS bert+spacy"></a>bert VS bert+spacy</h3><p>epoch &#x3D; 3</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616174343675.png" alt="image-20240616174343675"></p>
<p>epoch &#x3D; 10</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616175243506.png" alt="image-20240616175243506"></p>
<p>可以看到 融合了 句法特征确实增加了 bert模型的准确性</p>
<h2 id="Task3-gloVe"><a href="#Task3-gloVe" class="headerlink" title="Task3 - gloVe"></a>Task3 - gloVe</h2><p>完整代码见GitHub</p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616180109384.png" alt="image-20240616180109384"></p>
<h2 id="任务四-基于跨度的情感三元组抽取"><a href="#任务四-基于跨度的情感三元组抽取" class="headerlink" title="任务四 基于跨度的情感三元组抽取"></a>任务四 基于跨度的情感三元组抽取</h2><p>基于跨度的情感三元组抽取任务是自然语言处理（NLP）领域的一项任务，旨在从文本中提取出关于情感的三元组。这些三元组通常由三个部分组成：实体（Entity），情感词（Sentiment word），以及情感极性（Sentiment polarity）。基于跨度的方法主要利用文本中的不同跨度（即子句）来识别这些三元组。</p>
<h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>情感三元组抽取任务的目标是从给定的句子中识别出三元组 (entity, sentiment word, sentiment polarity)。例如，在句子“我喜欢这款手机的屏幕，但电池寿命很短。”中，可以识别出两个三元组：</p>
<ol>
<li>(“屏幕”, “喜欢”, “正面”)</li>
<li>(“电池寿命”, “很短”, “负面”)</li>
</ol>
<h3 id="基于跨度的方法"><a href="#基于跨度的方法" class="headerlink" title="基于跨度的方法"></a>基于跨度的方法</h3><p>基于跨度的方法首先在文本中生成所有可能的子句（跨度），然后使用特定的模型来选择最可能的子句，最后通过这些子句来提取情感三元组。</p>
<h4 id="步骤概述"><a href="#步骤概述" class="headerlink" title="步骤概述"></a>步骤概述</h4><ol>
<li><strong>生成跨度（Span Generation）</strong>：在文本中生成所有可能的子句（跨度），每个跨度由起始位置和结束位置确定。</li>
<li><strong>跨度选择（Span Selection）</strong>：使用模型选择最有可能包含情感信息的跨度。</li>
<li><strong>情感三元组抽取（Triplet Extraction）</strong>：从选定的跨度中提取实体、情感词和情感极性，形成情感三元组。</li>
</ol>
<h3 id="基于跨度方法示例"><a href="#基于跨度方法示例" class="headerlink" title="基于跨度方法示例"></a>基于跨度方法示例</h3><p>以下是一个基于跨度的情感三元组抽取的详细示例：</p>
<ol>
<li><p><strong>生成跨度</strong>：<br>对于句子“这款手机的屏幕很清晰，电池寿命很短。”，生成所有可能的跨度，如：</p>
<ul>
<li>“这款手机”</li>
<li>“的屏幕”</li>
<li>“很清晰”</li>
<li>“电池寿命”</li>
<li>“很短”</li>
<li>等等</li>
</ul>
</li>
<li><p><strong>跨度选择</strong>：<br>使用序列标注模型（如BERT等预训练模型）选择最有可能包含情感信息的跨度：</p>
<ul>
<li>选择“屏幕”</li>
<li>选择“电池寿命”</li>
</ul>
</li>
<li><p><strong>情感三元组抽取</strong>：<br>从选定的跨度中提取情感三元组：</p>
<ul>
<li>从“屏幕”中提取情感词“清晰”，极性“正面”</li>
<li>从“电池寿命”中提取情感词“短”，极性“负面”</li>
</ul>
</li>
</ol>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>以下是一个简单的Python代码示例，展示如何生成跨度并选择情感三元组：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> textblob <span class="keyword">import</span> TextBlob</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载英文模型</span></span><br><span class="line">nlp = spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例评论</span></span><br><span class="line">reviews = [</span><br><span class="line">    <span class="string">&quot;This laptop meets every expectation and Windows 7 is great!&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Drivers updated ok but the BIOS update froze the system up and the computer shut down.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;It rarely works and when it does it&#x27;s incredibly slow.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The battery life is amazing and the screen is very clear.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;The keyboard feels cheap and the touchpad is unresponsive.&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取情感三元组</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_sentiment_triplets</span>(<span class="params">reviews</span>):</span><br><span class="line">    triplets = []</span><br><span class="line">    <span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">        doc = nlp(review)</span><br><span class="line">        sentiment = TextBlob(review).sentiment.polarity</span><br><span class="line">        <span class="keyword">if</span> sentiment &gt; <span class="number">0</span>:</span><br><span class="line">            sentiment_label = <span class="string">&#x27;POS&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> sentiment &lt; <span class="number">0</span>:</span><br><span class="line">            sentiment_label = <span class="string">&#x27;NEG&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sentiment_label = <span class="string">&#x27;NEU&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(f&quot;Review: &#123;review&#125;&quot;)</span></span><br><span class="line">        <span class="comment"># print(f&quot;Overall Sentiment: &#123;sentiment_label&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取所有名词短语作为潜在的实体和属性</span></span><br><span class="line">        noun_phrases = [chunk.text <span class="keyword">for</span> chunk <span class="keyword">in</span> doc.noun_chunks]</span><br><span class="line">        <span class="comment"># print(f&quot;Noun Phrases: &#123;noun_phrases&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> chunk <span class="keyword">in</span> doc.noun_chunks:</span><br><span class="line">            noun_phrase = chunk.text</span><br><span class="line">            <span class="comment"># 查找修饰名词短语的形容词</span></span><br><span class="line">            adjectives = [token.text <span class="keyword">for</span> token <span class="keyword">in</span> chunk.root.head.children <span class="keyword">if</span> token.dep_ <span class="keyword">in</span> (<span class="string">&quot;amod&quot;</span>, <span class="string">&quot;acomp&quot;</span>)]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> adjective <span class="keyword">in</span> adjectives:</span><br><span class="line">                <span class="comment"># 分析形容词的情感</span></span><br><span class="line">                phrase_sentiment = TextBlob(adjective).sentiment.polarity</span><br><span class="line">                <span class="keyword">if</span> phrase_sentiment &gt; <span class="number">0</span>:</span><br><span class="line">                    phrase_sentiment_label = <span class="string">&#x27;POS&#x27;</span></span><br><span class="line">                <span class="keyword">elif</span> phrase_sentiment &lt; <span class="number">0</span>:</span><br><span class="line">                    phrase_sentiment_label = <span class="string">&#x27;NEG&#x27;</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    phrase_sentiment_label = <span class="string">&#x27;NEU&#x27;</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># print(f&quot;Noun Phrase: &#123;noun_phrase&#125;, Adjective: &#123;adjective&#125;, Sentiment: &#123;phrase_sentiment_label&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 只考虑与整体情感一致的名词短语</span></span><br><span class="line">                <span class="keyword">if</span> phrase_sentiment_label == sentiment_label:</span><br><span class="line">                    triplets.append((noun_phrase, adjective, sentiment_label))</span><br><span class="line">                    <span class="comment"># print(f&quot;Triplet: (&#123;noun_phrase&#125;, &#123;adjective&#125;, &#123;sentiment_label&#125;)&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> triplets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">triplets = extract_sentiment_triplets(reviews)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Extracted Sentiment Triplets:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> triplet <span class="keyword">in</span> triplets:</span><br><span class="line">    <span class="built_in">print</span>(triplet)</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Extracted Sentiment Triplets:</span><br><span class="line">(&#x27;it&#x27;, &#x27;slow&#x27;, &#x27;NEG&#x27;)</span><br><span class="line">(&#x27;The battery life&#x27;, &#x27;amazing&#x27;, &#x27;POS&#x27;)</span><br><span class="line">(&#x27;the screen&#x27;, &#x27;clear&#x27;, &#x27;POS&#x27;)</span><br><span class="line">(&#x27;The keyboard&#x27;, &#x27;cheap&#x27;, &#x27;POS&#x27;)</span><br></pre></td></tr></table></figure>



<h2 id="⌚️代码实践"><a href="#⌚️代码实践" class="headerlink" title="⌚️代码实践"></a>⌚️代码实践</h2><p>TextBlob 是一个简洁易用的 Python 库，用于处理文本数据。它基于 NLTK 和 Pattern 库，提供了便捷的 API 来执行常见的自然语言处理（NLP）任务，例如词性标注、名词短语提取、情感分析等。</p>
<h3 id="demo-spacy-textblob"><a href="#demo-spacy-textblob" class="headerlink" title="demo-spacy+textblob"></a>demo-spacy+textblob</h3><p>设计： </p>
<ul>
<li>很朴素的想法是找到 （主语+形容词+情感极性）</li>
<li>根据整体句子的情感极性为依据，过滤掉冲突的三元组</li>
<li>单词索引 – &gt; 首字符的位置</li>
</ul>
<p>Issue ：如何找到句子的所有表达情感的形容词的位置的跨度，以及其形容的主语的跨度，以及最后的情感极性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> textblob <span class="keyword">import</span> TextBlob</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载英文模型</span></span><br><span class="line">nlp = spacy.load(<span class="string">&#x27;en_core_web_sm&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例文本</span></span><br><span class="line">text = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">This laptop meets every expectation and Windows 7 is great!</span></span><br><span class="line"><span class="string">Drivers updated ok but the BIOS update froze the system up and the computer shut down.</span></span><br><span class="line"><span class="string">It rarely works and when it does it&#x27;s incredibly slow.</span></span><br><span class="line"><span class="string">The battery life is amazing and the screen is very clear.</span></span><br><span class="line"><span class="string">The keyboard feels cheap and the touchpad is unresponsive.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理文本</span></span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取情感形容词及其修饰的名词短语的跨度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_sentiment_adjectives</span>(<span class="params">doc</span>):</span><br><span class="line">    all_triplets = []</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> doc.sents:</span><br><span class="line">        sentence_triplets = []</span><br><span class="line">        <span class="comment"># 获取句子的整体情感</span></span><br><span class="line">        sentence_sentiment = TextBlob(sentence.text).sentiment.polarity</span><br><span class="line">        sentence_sentiment_label = <span class="string">&#x27;POS&#x27;</span> <span class="keyword">if</span> sentence_sentiment &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;NEG&#x27;</span> <span class="keyword">if</span> sentence_sentiment &lt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;NEU&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence:</span><br><span class="line">            <span class="keyword">if</span> token.pos_ == <span class="string">&#x27;ADJ&#x27;</span>:</span><br><span class="line">                <span class="comment"># 查找修饰的名词</span></span><br><span class="line">                noun = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">if</span> token.head.pos_ == <span class="string">&#x27;NOUN&#x27;</span>:</span><br><span class="line">                    noun = token.head</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">for</span> ancestor <span class="keyword">in</span> token.ancestors:</span><br><span class="line">                        <span class="keyword">if</span> ancestor.pos_ == <span class="string">&#x27;NOUN&#x27;</span>:</span><br><span class="line">                            noun = ancestor</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> noun <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">for</span> child <span class="keyword">in</span> token.children:</span><br><span class="line">                        <span class="keyword">if</span> child.dep_ <span class="keyword">in</span> (<span class="string">&#x27;nsubj&#x27;</span>, <span class="string">&#x27;dobj&#x27;</span>) <span class="keyword">and</span> child.pos_ == <span class="string">&#x27;NOUN&#x27;</span>:</span><br><span class="line">                            noun = child</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> noun <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">for</span> sibling <span class="keyword">in</span> token.head.children:</span><br><span class="line">                        <span class="keyword">if</span> sibling.pos_ == <span class="string">&#x27;NOUN&#x27;</span> <span class="keyword">and</span> sibling != token:</span><br><span class="line">                            noun = sibling</span><br><span class="line">                            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> noun:</span><br><span class="line">                    <span class="comment"># 获取形容词的情感极性</span></span><br><span class="line">                    adjective_sentiment = TextBlob(token.text).sentiment.polarity</span><br><span class="line">                    adjective_sentiment_label = <span class="string">&#x27;POS&#x27;</span> <span class="keyword">if</span> adjective_sentiment &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;NEG&#x27;</span> <span class="keyword">if</span> adjective_sentiment &lt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;NEU&#x27;</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 仅保留与句子情感一致的形容词</span></span><br><span class="line">                    <span class="keyword">if</span> adjective_sentiment_label == sentence_sentiment_label:</span><br><span class="line">                        <span class="comment"># 获取形容词和名词的词序起始位置</span></span><br><span class="line">                        adjective_index = token.i - sentence.start</span><br><span class="line">                        noun_index = noun.i - sentence.start</span><br><span class="line"></span><br><span class="line">                        sentence_triplets.append(<span class="string">f&quot;(<span class="subst">&#123;noun_index&#125;</span>, <span class="subst">&#123;adjective_index&#125;</span>, <span class="subst">&#123;adjective_sentiment_label&#125;</span>)&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sentence_triplets:</span><br><span class="line">            all_triplets.append((sentence.text.strip(), sentence_triplets))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> all_triplets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取并输出结果</span></span><br><span class="line">all_triplets = extract_sentiment_adjectives(doc)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Extracted Sentiment Triplets:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> sentence, triplets <span class="keyword">in</span> all_triplets:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence: <span class="subst">&#123;sentence&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Triplets:&quot;</span>, <span class="string">&quot; &quot;</span>.join(triplets))</span><br></pre></td></tr></table></figure>



<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616201225380.png" alt="image-20240616201225380"></p>
<h3 id="Dev-bert-tripletExtraction"><a href="#Dev-bert-tripletExtraction" class="headerlink" title="Dev bert-tripletExtraction"></a>Dev bert-tripletExtraction</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForTokenClassification</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TripletExtractionDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, tokenizer, max_len=<span class="number">128</span></span>):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.max_len = max_len</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        sentence, annotations = self.data[index]</span><br><span class="line">        encoding = self.tokenizer(sentence, max_length=self.max_len, truncation=<span class="literal">True</span>, padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">                                  return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">        input_ids = encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten()</span><br><span class="line">        attention_mask = encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten()</span><br><span class="line"></span><br><span class="line">        target_labels = [-<span class="number">100</span>] * self.max_len</span><br><span class="line">        opinion_labels = [-<span class="number">100</span>] * self.max_len</span><br><span class="line">        sentiment_labels = [-<span class="number">100</span>] * self.max_len</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (target_pos, opinion_pos, sentiment) <span class="keyword">in</span> annotations:</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> target_pos:</span><br><span class="line">                <span class="keyword">if</span> pos &lt; self.max_len:</span><br><span class="line">                    target_labels[pos] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> opinion_pos:</span><br><span class="line">                <span class="keyword">if</span> pos &lt; self.max_len:</span><br><span class="line">                    opinion_labels[pos] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> target_pos:</span><br><span class="line">                <span class="keyword">if</span> pos &lt; self.max_len:</span><br><span class="line">                    sentiment_labels[pos] = <span class="number">1</span> <span class="keyword">if</span> sentiment == <span class="string">&#x27;POS&#x27;</span> <span class="keyword">else</span> (<span class="number">2</span> <span class="keyword">if</span> sentiment == <span class="string">&#x27;NEG&#x27;</span> <span class="keyword">else</span> <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>: input_ids,</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>: attention_mask,</span><br><span class="line">            <span class="string">&#x27;target_labels&#x27;</span>: torch.tensor(target_labels, dtype=torch.long),</span><br><span class="line">            <span class="string">&#x27;opinion_labels&#x27;</span>: torch.tensor(opinion_labels, dtype=torch.long),</span><br><span class="line">            <span class="string">&#x27;sentiment_labels&#x27;</span>: torch.tensor(sentiment_labels, dtype=torch.long),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_annotations</span>(<span class="params">annotation_str</span>):</span><br><span class="line">    triplets = re.findall(<span class="string">r&#x27;\(\[(.*?)\], \[(.*?)\], \&#x27;(.*?)\&#x27;\)&#x27;</span>, annotation_str)</span><br><span class="line">    annotations = []</span><br><span class="line">    <span class="keyword">for</span> target_pos_str, opinion_pos_str, sentiment <span class="keyword">in</span> triplets:</span><br><span class="line">        target_pos = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, target_pos_str.split(<span class="string">&#x27;, &#x27;</span>)))</span><br><span class="line">        opinion_pos = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, opinion_pos_str.split(<span class="string">&#x27;, &#x27;</span>)))</span><br><span class="line">        annotations.append((target_pos, opinion_pos, sentiment))</span><br><span class="line">    <span class="keyword">return</span> annotations</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">file_path, tokenizer</span>):</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                sentence, annotation_str = line.strip().split(<span class="string">&#x27;####&#x27;</span>)</span><br><span class="line">                annotations = parse_annotations(annotation_str)</span><br><span class="line">                data.append((sentence, annotations))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;意外错误: <span class="subst">&#123;e&#125;</span> 在行: <span class="subst">&#123;line&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> TripletExtractionDataset(data, tokenizer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TripletExtractionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name</span>):</span><br><span class="line">        <span class="built_in">super</span>(TripletExtractionModel, self).__init__()</span><br><span class="line">        self.bert = BertForTokenClassification.from_pretrained(model_name,</span><br><span class="line">                                                               num_labels=<span class="number">4</span>)  <span class="comment"># 0: no label, 1: target, 2: opinion, 3: sentiment</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, target_labels=<span class="literal">None</span>, opinion_labels=<span class="literal">None</span>, sentiment_labels=<span class="literal">None</span></span>):</span><br><span class="line">        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=target_labels)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line"></span><br><span class="line">        opinion_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=opinion_labels)</span><br><span class="line">        loss += opinion_outputs.loss</span><br><span class="line"></span><br><span class="line">        sentiment_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=sentiment_labels)</span><br><span class="line">        loss += sentiment_outputs.loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, outputs.logits, opinion_outputs.logits, sentiment_outputs.logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, get_linear_schedule_with_warmup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, dataloader, optimizer, scheduler, num_epochs=<span class="number">3</span></span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">            attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">            target_labels = batch[<span class="string">&#x27;target_labels&#x27;</span>].to(device)</span><br><span class="line">            opinion_labels = batch[<span class="string">&#x27;opinion_labels&#x27;</span>].to(device)</span><br><span class="line">            sentiment_labels = batch[<span class="string">&#x27;sentiment_labels&#x27;</span>].to(device)</span><br><span class="line"></span><br><span class="line">            loss, _, _, _ = model(input_ids, attention_mask, target_labels, opinion_labels, sentiment_labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Step <span class="subst">&#123;step&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        avg_loss = total_loss / <span class="built_in">len</span>(dataloader)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, 平均损失: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, dataloader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line">    total_predictions = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataloader:</span><br><span class="line">        input_ids = batch[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">        target_labels = batch[<span class="string">&#x27;target_labels&#x27;</span>].to(device)</span><br><span class="line">        opinion_labels = batch[<span class="string">&#x27;opinion_labels&#x27;</span>].to(device)</span><br><span class="line">        sentiment_labels = batch[<span class="string">&#x27;sentiment_labels&#x27;</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            loss, target_logits, opinion_logits, sentiment_logits = model(input_ids, attention_mask, target_labels,</span><br><span class="line">                                                                          opinion_labels, sentiment_labels)</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            target_preds = torch.argmax(target_logits, dim=-<span class="number">1</span>)</span><br><span class="line">            opinion_preds = torch.argmax(opinion_logits, dim=-<span class="number">1</span>)</span><br><span class="line">            sentiment_preds = torch.argmax(sentiment_logits, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            correct_predictions += (target_preds == target_labels).<span class="built_in">sum</span>().item()</span><br><span class="line">            total_predictions += torch.<span class="built_in">sum</span>(target_labels != -<span class="number">100</span>).item()  <span class="comment"># 忽略填充部分</span></span><br><span class="line"></span><br><span class="line">    avg_loss = total_loss / <span class="built_in">len</span>(dataloader)</span><br><span class="line">    accuracy = correct_predictions / total_predictions</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;评估平均损失: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>, 准确率: <span class="subst">&#123;accuracy:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保设备分配正确</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;mps&#x27;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = TripletExtractionModel(<span class="string">&#x27;bert-base-uncased&#x27;</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">train_dataset = preprocess_data(</span><br><span class="line">    <span class="string">&#x27;/Users/bootscoder/PycharmProjects/nlp-lesson-design/data/SemEval-Triplet-data/ASTE-Data-V2-EMNLP2020/14lap/train_triplets.txt&#x27;</span>,</span><br><span class="line">    tokenizer)</span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">val_dataset = preprocess_data(</span><br><span class="line">    <span class="string">&#x27;/Users/bootscoder/PycharmProjects/nlp-lesson-design/data/SemEval-Triplet-data/ASTE-Data-V2-EMNLP2020/14lap/test_triplets.txt&#x27;</span>,</span><br><span class="line">    tokenizer)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置优化器和调度器</span></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>, correct_bias=<span class="literal">False</span>)</span><br><span class="line">total_steps = <span class="built_in">len</span>(train_dataloader) * <span class="number">3</span></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=<span class="number">0</span>, num_training_steps=total_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">train_model(model, train_dataloader, optimizer, scheduler, num_epochs=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">evaluate_model(model, val_dataloader)</span><br></pre></td></tr></table></figure>



<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240616201132315.png" alt="image-20240616201132315"></p>
<h2 id="SSJE"><a href="#SSJE" class="headerlink" title="SSJE"></a>SSJE</h2><ul>
<li>翻新GitHub代码</li>
<li>修改为mps + python 11</li>
</ul>
<p>完整代码见 GitHub </p>
<p>参考<a target="_blank" rel="noopener" href="https://github.com/MKMaS-GUET/SSJE?tab=readme-ov-file">https://github.com/MKMaS-GUET/SSJE?tab=readme-ov-file</a></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617041934398.png" alt="image-20240617041934398"></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617043134155.png" alt="image-20240617043134155"></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617043210161.png" alt="image-20240617043210161"></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617044017009.png" alt="image-20240617044017009"></p>
<p><img src="https://markdown-pictures-jhx.oss-cn-beijing.aliyuncs.com/picgo/image-20240617043935298.png" alt="image-20240617043935298"></p>
<h2 id="spacy-gloVe-bert"><a href="#spacy-gloVe-bert" class="headerlink" title="spacy+ gloVe+ bert"></a>spacy+ gloVe+ bert</h2><p>我现在有两个glove_vectors_list每个中含有若干向量， 向量的维度是100d，我已经得到了这两个sequence的情感分类的标签</p>
<p>labels &#x3D; [1, 1]</p>
<p> 1 表示 pos； 0 表示 neu ；-1 表示neg</p>
<p> 请你基于我已经分好的glove_vectors_list 和对应的标签 计算他们内的 向量的self attention 得分矩阵 并且找到 和标签的语意最接近（得分最高）的 一组短语的向量和短语 ； 再找到和该短语最接近的 一组主语；</p>
<p>最终会返回一个 三元组[([第一个向量的序号], [第二个向量的序号], ‘POS’)] 其中的列表表示该短语以单词为单位的跨度</p>
<p>完整代码见 GitHub</p>

      

      
        <div class="page-reward">
          <a href="javascript:;" class="page-reward-btn tooltip-top">
            <div class="tooltip tooltip-east">
            <span class="tooltip-item">
              赏
            </span>
            <span class="tooltip-content">
              <span class="tooltip-text">
                <span class="tooltip-inner">
                  <p class="reward-p"><i class="icon icon-quo-left"></i>a cup of coofee<i class="icon icon-quo-right"></i></p>
                  <div class="reward-box">
                    
                    
                    <div class="reward-box-item">
                      <img class="reward-img" src="/img/wepay.jpg">
                      <span class="reward-type">微信</span>
                    </div>
                    
                  </div>
                </span>
              </span>
            </span>
          </div>
          </a>
        </div>
      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">nlp</a>
        		</li>
      		
		</ul>
	</div>

      

      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/2024/06/19/%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B7%AF/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          算法之路
        
      </div>
    </a>
  
  
    <a href="/2024/06/14/test/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">test</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp-%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.</span> <span class="toc-text">nlp 课程设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#GloVE"><span class="toc-number">1.1.</span> <span class="toc-text">GloVE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%BC%95%E8%A8%80%EF%BC%9A"><span class="toc-number">1.1.1.</span> <span class="toc-text">概述与引言：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E4%B8%8E%E4%BC%98%E5%8A%BF%EF%BC%9A"><span class="toc-number">1.1.2.</span> <span class="toc-text">基本思想与优势：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-work"><span class="toc-number">1.1.3.</span> <span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC%EF%BC%9A"><span class="toc-number">1.1.4.</span> <span class="toc-text">核心思想与数学推导：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E2%80%93-Example-of-Co-occurrence-Matrices"><span class="toc-number">1.1.5.</span> <span class="toc-text">扩展– Example of Co-occurrence Matrices</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E2%80%93%E7%86%B5%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.6.</span> <span class="toc-text">扩展–熵归一化的具体操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E2%80%93-%E5%8F%8C%E7%BA%BF%E5%BD%A2%E5%AF%B9%E6%95%B0%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.7.</span> <span class="toc-text">扩展– 双线形对数回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT"><span class="toc-number">1.2.</span> <span class="toc-text">BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.2.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-%E7%9A%84%E7%BD%91%E8%B7%AF%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.2.</span> <span class="toc-text">bert 的网路结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94"><span class="toc-number">1.2.3.</span> <span class="toc-text">对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-number">1.2.4.</span> <span class="toc-text">参数计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pre-training-and-fine-tuning"><span class="toc-number">1.2.5.</span> <span class="toc-text">pre-training and fine tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pretrain"><span class="toc-number">1.2.6.</span> <span class="toc-text">pretrain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fine-tuning"><span class="toc-number">1.2.7.</span> <span class="toc-text">Fine tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ablation-Studies"><span class="toc-number">1.2.8.</span> <span class="toc-text">Ablation Studies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%B8%80-%E5%9F%BA%E4%BA%8E%E6%96%B9%E9%9D%A2%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%9A%84%E8%83%8C%E6%99%AF%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.3.</span> <span class="toc-text">任务一 基于方面情感分析的背景表示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%82%E8%80%83"><span class="toc-number">1.3.1.</span> <span class="toc-text">数据集参考</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8bert-%E8%83%8C%E6%99%AF%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.3.2.</span> <span class="toc-text">使用bert- 背景表示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E6%80%BB%E7%BB%93"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">嵌入总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-gloVE-%E2%80%93-%E8%83%8C%E6%99%AF%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.3.3.</span> <span class="toc-text">使用 gloVE – 背景表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GloVe-vs-BERT"><span class="toc-number">1.3.4.</span> <span class="toc-text">GloVe vs. BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BA%8C-%E5%9F%BA%E4%BA%8E%E6%96%B9%E9%9D%A2%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%8F%A5%E6%B3%95%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.4.</span> <span class="toc-text">任务二 基于方面情感分析任务的句法特征表示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tool"><span class="toc-number">1.4.1.</span> <span class="toc-text">tool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#demo"><span class="toc-number">1.4.2.</span> <span class="toc-text">demo</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8"><span class="toc-number">1.4.3.</span> <span class="toc-text">作用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%B8%89-%E5%9F%BA%E4%BA%8E%E6%96%B9%E9%9D%A2%E6%83%85%E6%84%9F%E2%80%93%E8%83%8C%E6%99%AF%E8%A1%A8%E7%A4%BA%E5%92%8C%E5%8F%A5%E6%B3%95%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E7%9A%84%E8%9E%8D%E5%90%88"><span class="toc-number">1.5.</span> <span class="toc-text">任务三 基于方面情感–背景表示和句法特征表示的融合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.5.1.</span> <span class="toc-text">设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.2.</span> <span class="toc-text">融合方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">方法一</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">方法二</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#demo-1"><span class="toc-number">1.5.3.</span> <span class="toc-text">demo</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%EF%BC%9A-%E4%BD%BF%E7%94%A8-random-forest"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">完整代码： 使用 random forest</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8-bi-lstm-attention"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">完整代码使用 bi -lstm + attention</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Task3-Bert"><span class="toc-number">1.6.</span> <span class="toc-text">Task3 - Bert</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%BC%8F%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86-%E2%80%93-bert"><span class="toc-number">1.6.1.</span> <span class="toc-text">正式处理数据集 – bert</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%BC%8F%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86-%E2%80%93-bert-spacy"><span class="toc-number">1.6.2.</span> <span class="toc-text">正式处理数据集 – bert+spacy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bert-VS-bert-spacy"><span class="toc-number">1.6.3.</span> <span class="toc-text">bert VS bert+spacy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Task3-gloVe"><span class="toc-number">1.7.</span> <span class="toc-text">Task3 - gloVe</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E5%9B%9B-%E5%9F%BA%E4%BA%8E%E8%B7%A8%E5%BA%A6%E7%9A%84%E6%83%85%E6%84%9F%E4%B8%89%E5%85%83%E7%BB%84%E6%8A%BD%E5%8F%96"><span class="toc-number">1.8.</span> <span class="toc-text">任务四 基于跨度的情感三元组抽取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E5%AE%9A%E4%B9%89"><span class="toc-number">1.8.1.</span> <span class="toc-text">任务定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%B7%A8%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.8.2.</span> <span class="toc-text">基于跨度的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E6%A6%82%E8%BF%B0"><span class="toc-number">1.8.2.1.</span> <span class="toc-text">步骤概述</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%B7%A8%E5%BA%A6%E6%96%B9%E6%B3%95%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.8.3.</span> <span class="toc-text">基于跨度方法示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.8.4.</span> <span class="toc-text">代码示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%8C%9A%EF%B8%8F%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.9.</span> <span class="toc-text">⌚️代码实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#demo-spacy-textblob"><span class="toc-number">1.9.1.</span> <span class="toc-text">demo-spacy+textblob</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dev-bert-tripletExtraction"><span class="toc-number">1.9.2.</span> <span class="toc-text">Dev bert-tripletExtraction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSJE"><span class="toc-number">1.10.</span> <span class="toc-text">SSJE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spacy-gloVe-bert"><span class="toc-number">1.11.</span> <span class="toc-text">spacy+ gloVe+ bert</span></a></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  
  
  

  

  

  

<!-- 《utteranc评论：基于github issue的评论系统 -->

	
		<section id="comments" class="comments">
		  <style>
			.utterances{max-width: 100%;}
		  </style>
			<script src="https://utteranc.es/client.js"
					repo="boots-coder/boots-coder.github.io"
					issue-term="pathname"
					theme="github-light"
					crossorigin="anonymous"
					async>
			</script>
	  </section>
	

<!-- utteranc评论》 -->

          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2024 coderboots
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: true
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">随笔</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">测试</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">nlp</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">算法</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">勤能补拙&lt;br&gt;&lt;br&gt;笔耕不辍，乐在其中&lt;br&gt;&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>